import express from 'express';
import cors from 'cors';
import helmet from 'helmet';
import compression from 'compression';
import { createServer } from 'http';
import { WebSocketServer } from 'ws';
import { v4 as uuidv4 } from 'uuid';
import { AIService } from './services/ai-service';
import { ValidationService } from './services/validation-service';
import { MonitoringService } from './services/monitoring-service';
import { CacheService } from './services/cache-service';
import { TemplateService } from './services/template-service';
import { ParallelProcessor } from './services/parallel-processor';
import { PreGenerationService } from './services/pre-generation-service';
import { ConnectionPool } from './services/connection-pool';
import { PredictionService } from './services/prediction-service';
import { UltraFastMiddleware } from './middleware/ultra-fast-middleware';
import { StreamOptimizer } from './utils/stream-optimizer';
import { Message, ChatSession, APIResponse } from './types';
import { SYSTEM_PROMPTS } from './config/prompts';

export class ChatApp {
  private app: express.Application;
  private server: any;
  private wss: WebSocketServer;
  private aiService: AIService;
  private validationService: ValidationService;
  private monitoringService: MonitoringService;
  private cacheService: CacheService;
  private templateService: TemplateService;
  private parallelProcessor: ParallelProcessor;
  private preGenService: PreGenerationService;
  private connectionPool: ConnectionPool;
  private predictionService: PredictionService;
  private ultraFastMiddleware: UltraFastMiddleware;
  private streamOptimizer: StreamOptimizer;
  private sessions: Map<string, ChatSession>;

  constructor() {
    this.app = express();
    this.server = createServer(this.app);
    this.wss = new WebSocketServer({ server: this.server });

    // Initialize core services
    this.aiService = new AIService();
    this.validationService = new ValidationService();
    this.monitoringService = new MonitoringService();

    // Initialize ultra-fast services
    this.cacheService = new CacheService();
    this.templateService = new TemplateService();
    this.connectionPool = new ConnectionPool();

    // Initialize advanced services with dependencies
    this.parallelProcessor = new ParallelProcessor(
      this.aiService,
      this.cacheService,
      this.templateService
    );
    this.preGenService = new PreGenerationService(
      this.aiService,
      this.cacheService
    );
    this.predictionService = new PredictionService(this.cacheService);

    // Initialize middleware and optimizers
    this.ultraFastMiddleware = new UltraFastMiddleware(
      this.cacheService,
      this.templateService,
      this.preGenService
    );
    this.streamOptimizer = new StreamOptimizer({
      bufferSize: 2048,
      chunkSize: 32,
      enableBatching: true,
      batchSize: 20,
      batchTimeout: 2
    });

    this.sessions = new Map();

    this.setupMiddleware();
    this.setupRoutes();
    this.setupWebSocket();
  }

  private setupMiddleware(): void {
    this.app.use(helmet());
    this.app.use(cors({
      origin: process.env.FRONTEND_URL || 'http://localhost:3000',
      credentials: true
    }));
    this.app.use(compression({
      level: 9,
      threshold: 1024
    }));

    // Ultra-fast middleware
    this.app.use(this.ultraFastMiddleware.handle());
    this.app.use(this.ultraFastMiddleware.responseHandler());

    this.app.use(express.json({ limit: '50mb' }));
    this.app.use(express.urlencoded({ extended: true }));

    // Request logging with performance tracking
    this.app.use((req, res, next) => {
      const start = process.hrtime.bigint();
      res.on('finish', () => {
        const end = process.hrtime.bigint();
        const duration = Number(end - start) / 1000000; // Convert to microseconds
        console.log(`âš¡ ${req.method} ${req.path} - ${res.statusCode} - ${duration.toFixed(2)}Î¼s`);
      });
      next();
    });
  }

  private setupRoutes(): void {
    // Health check
    this.app.get('/health', (req, res) => {
      res.json({
        status: 'healthy',
        timestamp: new Date(),
        uptime: process.uptime()
      });
    });

    // Get available providers and models
    this.app.get('/api/providers', async (req, res) => {
      try {
        const providers = await this.aiService.getAvailableProviders();
        res.json({ success: true, data: providers });
      } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
      }
    });

    // Get models for a specific provider
    this.app.get('/api/providers/:provider/models', async (req, res) => {
      try {
        const { provider } = req.params;
        const models = await this.aiService.getModelsByProvider(provider);
        res.json({ success: true, data: models });
      } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
      }
    });

    // Create new chat session
    this.app.post('/api/sessions', async (req, res) => {
      try {
        const { provider = 'OpenAI', model = 'gpt-3.5-turbo', userId } = req.body;
        const sessionId = uuidv4();

        const session: ChatSession = {
          id: sessionId,
          userId,
          messages: [],
          createdAt: new Date(),
          updatedAt: new Date(),
          metadata: {
            totalTokens: 0,
            averageLatency: 0,
            averageAuthenticity: 0,
            messageCount: 0,
            provider,
            model
          }
        };

        this.sessions.set(sessionId, session);

        res.json({
          success: true,
          data: {
            sessionId,
            provider,
            model,
            systemPrompt: SYSTEM_PROMPTS.HYPER_REALISTIC
          }
        });
      } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
      }
    });

    // Get session details
    this.app.get('/api/sessions/:sessionId', (req, res) => {
      try {
        const { sessionId } = req.params;
        const session = this.sessions.get(sessionId);

        if (!session) {
          return res.status(404).json({ success: false, error: 'Session not found' });
        }

        res.json({ success: true, data: session });
      } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
      }
    });

    // Ultra-fast message endpoint with all optimizations
    this.app.post('/api/sessions/:sessionId/messages', async (req, res) => {
      try {
        const { sessionId } = req.params;
        const { message, provider, model, speed = 'ultra', systemPrompt } = req.body;

        const session = this.sessions.get(sessionId);
        if (!session) {
          return res.status(404).json({ success: false, error: 'Session not found' });
        }

        const startTime = process.hrtime.bigint();

        // Add user message
        const userMessage: Message = {
          id: uuidv4(),
          role: 'user',
          content: message,
          timestamp: new Date()
        };
        session.messages.push(userMessage);

        // Try ultra-fast paths first
        let response = '';
        let source = 'unknown';
        let authenticityScore = 0.8;
        let fakeScore = 0.2;

        // 1. Check prediction (fastest)
        const prediction = await this.predictionService.predictResponse(
          message,
          session.messages.slice(0, -1),
          session.messages
        );
        if (prediction && prediction.confidence > 0.7) {
          response = prediction.text;
          source = prediction.source;
          authenticityScore = 0.9;
          fakeScore = 0.1;
        } else {
          // 2. Try parallel processing (very fast)
          const parallelResult = await this.parallelProcessor.processRequest(
            session.messages,
            model || session.metadata.model,
            'ultra'
          );
          response = parallelResult.response;
          source = parallelResult.source;
          authenticityScore = 0.85;
          fakeScore = 0.15;
        }

        const endTime = process.hrtime.bigint();
        const latency = Number(endTime - startTime) / 1000000; // Convert to milliseconds

        // Add AI message
        const aiMessage: Message = {
          id: uuidv4(),
          role: 'assistant',
          content: response,
          timestamp: new Date(),
          metadata: {
            model: model || session.metadata.model,
            provider: provider || session.metadata.provider,
            latency: parseFloat(latency.toFixed(2)),
            source,
            authenticityScore,
            fakeScore,
            confidence: 0.9
          }
        };
        session.messages.push(aiMessage);

        // Validate response
        const validation = await this.validationService.validateResponse(
          response,
          session.messages.slice(0, -1),
          message
        );

        // Update message metadata with validation
        aiMessage.metadata = {
          ...aiMessage.metadata,
          authenticityScore: validation.authenticityResult.score,
          fakeScore: validation.fakeDetectionResult.confidence,
          confidence: validation.authenticityResult.confidence
        };

        // Update session metadata
        session.metadata.totalTokens += await this.aiService.estimateTokens(response);
        session.metadata.averageLatency =
          (session.metadata.averageLatency * (session.metadata.messageCount - 1) + latency) /
          session.metadata.messageCount;
        session.metadata.averageAuthenticity =
          (session.metadata.averageAuthenticity * (session.metadata.messageCount - 1) +
          validation.authenticityResult.score) / session.metadata.messageCount;
        session.metadata.messageCount++;
        session.updatedAt = new Date();

        // Track metrics
        await this.monitoringService.trackRequest(
          sessionId,
          provider || session.metadata.provider,
          model || session.metadata.model,
          parseFloat(latency.toFixed(2)),
          await this.aiService.estimateTokens(response),
          validation.authenticityResult.score,
          validation.fakeDetectionResult.confidence
        );

        // Learn from interaction
        await this.predictionService.learnFromInteraction(
          message,
          response,
          prediction?.text
        );

        // Cache response for future
        await this.cacheService.cacheResponsePattern(
          session.messages,
          response,
          validation.authenticityResult.score
        );

        res.json({
          success: true,
          data: {
            message: aiMessage,
            validation: {
              authenticity: validation.authenticityResult,
              fakeDetection: validation.fakeDetectionResult,
              shouldRetry: validation.shouldRetry
            },
            performance: {
              latency: parseFloat(latency.toFixed(2)),
              source,
              tokensPerSecond: Math.round((response.length / parseFloat(latency.toFixed(2))) * 1000)
            }
          }
        });
      } catch (error: any) {
        console.error('Ultra-fast message error:', error);
        res.status(500).json({ success: false, error: error.message });
      }
    });

    // Legacy endpoint (non-streaming)
    this.app.post('/api/sessions/:sessionId/messages/legacy', async (req, res) => {
      try {
        const { sessionId } = req.params;
        const { message, provider, model, speed = 'ultra', systemPrompt } = req.body;

        const session = this.sessions.get(sessionId);
        if (!session) {
          return res.status(404).json({ success: false, error: 'Session not found' });
        }

        const startTime = Date.now();

        // Add user message
        const userMessage: Message = {
          id: uuidv4(),
          role: 'user',
          content: message,
          timestamp: new Date()
        };
        session.messages.push(userMessage);

        // Generate AI response
        const responseStream = await this.aiService.generateResponse(
          session.messages,
          model || session.metadata.model,
          { speed, tokensPerSecond: 10000, chunkSize: 1, enableLineByLine: true },
          systemPrompt
        );

        let response = '';
        for await (const chunk of responseStream) {
          response += chunk;
        }

        const endTime = Date.now();
        const latency = endTime - startTime;

        // Add AI message
        const aiMessage: Message = {
          id: uuidv4(),
          role: 'assistant',
          content: response,
          timestamp: new Date(),
          metadata: {
            model: model || session.metadata.model,
            provider: provider || session.metadata.provider,
            latency
          }
        };
        session.messages.push(aiMessage);

        // Validate response
        const validation = await this.validationService.validateResponse(
          response,
          session.messages.slice(0, -1),
          message
        );

        // Update message metadata
        aiMessage.metadata = {
          ...aiMessage.metadata,
          authenticityScore: validation.authenticityResult.score,
          fakeScore: validation.fakeDetectionResult.confidence,
          confidence: validation.authenticityResult.confidence
        };

        // Update session metadata
        session.metadata.totalTokens += await this.aiService.estimateTokens(response);
        session.metadata.averageLatency =
          (session.metadata.averageLatency * (session.metadata.messageCount - 1) + latency) /
          session.metadata.messageCount;
        session.metadata.averageAuthenticity =
          (session.metadata.averageAuthenticity * (session.metadata.messageCount - 1) +
          validation.authenticityResult.score) / session.metadata.messageCount;
        session.metadata.messageCount++;
        session.updatedAt = new Date();

        // Track metrics
        await this.monitoringService.trackRequest(
          sessionId,
          provider || session.metadata.provider,
          model || session.metadata.model,
          latency,
          await this.aiService.estimateTokens(response),
          validation.authenticityResult.score,
          validation.fakeDetectionResult.confidence
        );

        res.json({
          success: true,
          data: {
            message: aiMessage,
            validation: {
              authenticity: validation.authenticityResult,
              fakeDetection: validation.fakeDetectionResult,
              shouldRetry: validation.shouldRetry
            }
          }
        });
      } catch (error: any) {
        console.error('Message error:', error);
        res.status(500).json({ success: false, error: error.message });
      }
    });

    // Get validation report
    this.app.post('/api/validate', async (req, res) => {
      try {
        const { response, conversationHistory, userMessage } = req.body;

        const report = await this.validationService.getValidationReport(
          response,
          conversationHistory,
          userMessage
        );

        res.json({ success: true, data: report });
      } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
      }
    });

    // Get monitoring metrics
    this.app.get('/api/metrics', async (req, res) => {
      try {
        const { timeRange } = req.query;
        const metrics = await this.monitoringService.getMetrics(
          timeRange as any
        );
        res.json({ success: true, data: metrics });
      } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
      }
    });

    // Generate monitoring report
    this.app.get('/api/reports', async (req, res) => {
      try {
        const { timeRange = 'day' } = req.query;
        const report = await this.monitoringService.generateReport(
          timeRange as any
        );
        res.json({ success: true, data: report });
      } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
      }
    });

    // Get top providers
    this.app.get('/api/providers/top', async (req, res) => {
      try {
        const { limit = 10 } = req.query;
        const topProviders = await this.monitoringService.getTopProviders(
          Number(limit)
        );
        res.json({ success: true, data: topProviders });
      } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
      }
    });

    // Ultra-fast batch message processing
    this.app.post('/api/sessions/:sessionId/batch', async (req, res) => {
      try {
        const { sessionId } = req.params;
        const { messages } = req.body;

        const session = this.sessions.get(sessionId);
        if (!session) {
          return res.status(404).json({ success: false, error: 'Session not found' });
        }

        const startTime = process.hrtime.bigint();
        const responses = [];

        // Process in parallel with all optimizations
        const batchPromises = messages.map(async (msg: any) => {
          // Check cache first
          const cached = await this.cacheService.getResponsePattern([
            ...session.messages,
            { role: 'user', content: msg, id: '', timestamp: new Date() }
          ]);
          if (cached) {
            return {
              message: cached,
              source: 'cache',
              latency: 0
            };
          }

          // Use parallel processing
          const result = await this.parallelProcessor.processRequest(
            [...session.messages, { role: 'user', content: msg, id: '', timestamp: new Date() }],
            session.metadata.model,
            'ultra'
          );

          return {
            message: result.response,
            source: result.source,
            latency: result.latency
          };
        });

        const batchResults = await Promise.all(batchPromises);
        const endTime = process.hrtime.bigint();
        const totalLatency = Number(endTime - startTime) / 1000000;

        // Add all messages to session
        for (let i = 0; i < messages.length; i++) {
          const userMessage: Message = {
            id: uuidv4(),
            role: 'user',
            content: messages[i],
            timestamp: new Date()
          };
          session.messages.push(userMessage);

          const aiMessage: Message = {
            id: uuidv4(),
            role: 'assistant',
            content: batchResults[i].message,
            timestamp: new Date(),
            metadata: {
              model: session.metadata.model,
              provider: session.metadata.provider,
              latency: batchResults[i].latency,
              source: batchResults[i].source
            }
          };
          session.messages.push(aiMessage);

          responses.push({
            user: userMessage,
            assistant: aiMessage
          });
        }

        res.json({
          success: true,
          data: {
            responses,
            performance: {
              totalLatency: parseFloat(totalLatency.toFixed(2)),
              averageLatency: parseFloat((totalLatency / messages.length).toFixed(2)),
              sources: batchResults.map(r => r.source)
            }
          }
        });
      } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
      }
    });

    // Predictive response endpoint
    this.app.post('/api/predict', async (req, res) => {
      try {
        const { input, context } = req.body;

        const startTime = process.hrtime.bigint();
        const prediction = await this.predictionService.predictResponse(input, context);
        const endTime = process.hrtime.bigint();
        const latency = Number(endTime - startTime) / 1000000;

        if (prediction) {
          res.json({
            success: true,
            data: {
              prediction: prediction.text,
              confidence: prediction.confidence,
              source: prediction.source,
              latency: parseFloat(latency.toFixed(2))
            }
          });
        } else {
          res.json({
            success: false,
            error: 'No prediction available'
          });
        }
      } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
      }
    });

    // Pre-generate common responses
    this.app.post('/api/pre-generate', async (req, res) => {
      try {
        const { patterns } = req.body;

        await this.preGenService.preGenerateFromHistory(
          patterns.map((p: any) => ({
            role: 'user',
            content: p,
            id: '',
            timestamp: new Date()
          }))
        );

        res.json({
          success: true,
          data: {
            message: 'Pre-generation started',
            patternsCount: patterns.length
          }
        });
      } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
      }
    });

    // Get optimization stats
    this.app.get('/api/optimization/stats', async (req, res) => {
      try {
        const stats = {
          cache: await this.cacheService.getStats(),
          templates: this.templateService.getTemplateStats(),
          parallel: this.parallelProcessor.getStats(),
          preGen: this.preGenService.getStats(),
          prediction: this.predictionService.getPredictionStats(),
          connections: this.connectionPool.getStats(),
          stream: this.streamOptimizer.getStats()
        };

        res.json({ success: true, data: stats });
      } catch (error: any) {
        res.status(500).json({ success: false, error: error.message });
      }
    });

    // Error handling middleware
    this.app.use((err: any, req: express.Request, res: express.Response, next: express.NextFunction) => {
      console.error('Unhandled error:', err);
      res.status(500).json({
        success: false,
        error: 'Internal server error'
      });
    });

    // 404 handler
    this.app.use((req, res) => {
      res.status(404).json({
        success: false,
        error: 'Route not found'
      });
    });
  }

  private setupWebSocket(): void {
    // Apply ultra-fast WebSocket optimizations
    this.wss.on('connection', (ws: any, req: any) => {
      const sessionId = req.url?.split('/ws/')[1];

      if (!sessionId || !this.sessions.has(sessionId)) {
        ws.close(1008, 'Invalid or missing session ID');
        return;
      }

      console.log(`âš¡ WebSocket connected for session: ${sessionId}`);

      // Apply ultra-fast optimizations
      this.streamOptimizer.createWSHandler()(ws, req);

      ws.on('message', async (data: any) => {
        try {
          const message = JSON.parse(data);

          if (message.type === 'chat') {
            await this.handleWebSocketMessage(ws, sessionId, message);
          } else if (message.type === 'ping') {
            ws.send(JSON.stringify({ type: 'pong' }));
          } else if (message.type === 'batch') {
            // Handle batch messages for ultra-fast processing
            await this.handleBatchWebSocketMessage(ws, sessionId, message);
          }
        } catch (error: any) {
          console.error('WebSocket error:', error);
          ws.send(JSON.stringify({
            type: 'error',
            message: error.message
          }));
        }
      });

      ws.on('close', () => {
        console.log(`WebSocket disconnected for session: ${sessionId}`);
      });

      ws.on('error', (error: any) => {
        console.error('WebSocket error:', error);
      });
    });
  }

  private async handleWebSocketMessage(
    ws: any,
    sessionId: string,
    message: any
  ): Promise<void> {
    const session = this.sessions.get(sessionId);
    if (!session) return;

    const startTime = process.hrtime.bigint();

    // Add user message
    const userMessage: Message = {
      id: uuidv4(),
      role: 'user',
      content: message.content,
      timestamp: new Date()
    };
    session.messages.push(userMessage);

    // Notify client that processing started
    ws.send(JSON.stringify({
      type: 'start',
      messageId: userMessage.id
    }));

    try {
      // Try ultra-fast paths first
      let response = '';
      let source = 'ai';
      let authenticityScore = 0.8;
      let fakeScore = 0.2;

      // 1. Check prediction (fastest)
      const prediction = await this.predictionService.predictResponse(
        message.content,
        session.messages.slice(0, -1),
        session.messages
      );
      if (prediction && prediction.confidence > 0.7) {
        response = prediction.text;
        source = 'prediction';
        authenticityScore = 0.9;
        fakeScore = 0.1;

        // Send immediate response
        const endTime = process.hrtime.bigint();
        const latency = Number(endTime - startTime) / 1000000;

        ws.send(JSON.stringify({
          type: 'complete',
          messageId: uuidv4(),
          response,
          metadata: {
            source,
            latency: parseFloat(latency.toFixed(2)),
            authenticityScore,
            fakeScore,
            confidence: prediction.confidence
          }
        }));

        return;
      }

      // 2. Use parallel processing for ultra speed
      const parallelResult = await this.parallelProcessor.processRequest(
        session.messages,
        message.model || session.metadata.model,
        'ultra'
      );
      response = parallelResult.response;
      source = parallelResult.source;
      authenticityScore = 0.85;
      fakeScore = 0.15;

      // Stream the response
      const responseStream = this.createStringStream(response);
      let partialResponse = '';
      let partialValidation = null;

      for await (const chunk of responseStream) {
        partialResponse += chunk;

        // Validate partial response
        partialValidation = await this.validationService.validateStreamingChunk(
          chunk,
          partialResponse,
          session.messages.slice(0, -1),
          message.content
        );

        // Send chunk with ultra-fast optimization
        ws.send(JSON.stringify({
          type: 'chunk',
          content: chunk,
          authenticityScore: partialValidation.authenticityScore,
          fakeScore: partialValidation.fakeScore,
          source
        }));
      }

      const endTime = process.hrtime.bigint();
      const latency = Number(endTime - startTime) / 1000000;

      // Final validation
      const finalValidation = await this.validationService.validateResponse(
        response,
        session.messages.slice(0, -1),
        message.content
      );

      // Add AI message
      const aiMessage: Message = {
        id: uuidv4(),
        role: 'assistant',
        content: response,
        timestamp: new Date(),
        metadata: {
          model: message.model || session.metadata.model,
          provider: message.provider || session.metadata.provider,
          latency: parseFloat(latency.toFixed(2)),
          source,
          authenticityScore: finalValidation.authenticityResult.score,
          fakeScore: finalValidation.fakeDetectionResult.confidence,
          confidence: finalValidation.authenticityResult.confidence
        }
      };
      session.messages.push(aiMessage);

      // Update session metadata
      session.metadata.totalTokens += await this.aiService.estimateTokens(response);
      session.metadata.averageLatency =
        (session.metadata.averageLatency * (session.metadata.messageCount - 1) + latency) /
        session.metadata.messageCount;
      session.metadata.averageAuthenticity =
        (session.metadata.averageAuthenticity * (session.metadata.messageCount - 1) +
        finalValidation.authenticityResult.score) / session.metadata.messageCount;
      session.metadata.messageCount++;
      session.updatedAt = new Date();

      // Track metrics
      await this.monitoringService.trackRequest(
        sessionId,
        message.provider || session.metadata.provider,
        message.model || session.metadata.model,
        parseFloat(latency.toFixed(2)),
        await this.aiService.estimateTokens(response),
        finalValidation.authenticityResult.score,
        finalValidation.fakeDetectionResult.confidence
      );

      // Learn from interaction
      await this.predictionService.learnFromInteraction(
        message.content,
        response,
        prediction?.text
      );

      // Cache response
      await this.cacheService.cacheResponsePattern(
        session.messages,
        response,
        finalValidation.authenticityResult.score
      );

      // Send completion message
      ws.send(JSON.stringify({
        type: 'complete',
        messageId: aiMessage.id,
        metadata: aiMessage.metadata,
        validation: {
          authenticity: finalValidation.authenticityResult,
          fakeDetection: finalValidation.fakeDetectionResult,
          shouldRetry: finalValidation.shouldRetry
        }
      }));

    } catch (error: any) {
      console.error('Ultra-fast WebSocket error:', error);
      ws.send(JSON.stringify({
        type: 'error',
        message: error.message
      }));
    }
  }

  // Ultra-fast batch WebSocket message handler
  private async handleBatchWebSocketMessage(
    ws: any,
    sessionId: string,
    message: any
  ): Promise<void> {
    const session = this.sessions.get(sessionId);
    if (!session) return;

    const { messages } = message;
    const startTime = process.hrtime.bigint();

    // Process all messages in parallel with optimizations
    const batchPromises = messages.map(async (msg: any) => {
      // Add user message
      const userMessage: Message = {
        id: uuidv4(),
        role: 'user',
        content: msg,
        timestamp: new Date()
      };

      // Try all ultra-fast paths
      const cached = await this.cacheService.getResponsePattern([
        ...session.messages,
        userMessage
      ]);

      if (cached) {
        return {
          user: userMessage,
          assistant: {
            id: uuidv4(),
            role: 'assistant',
            content: cached,
            timestamp: new Date(),
            metadata: { source: 'cache', latency: 0 }
          }
        };
      }

      // Use parallel processing
      const result = await this.parallelProcessor.processRequest(
        [...session.messages, userMessage],
        session.metadata.model,
        'ultra'
      );

      return {
        user: userMessage,
        assistant: {
          id: uuidv4(),
          role: 'assistant',
          content: result.response,
          timestamp: new Date(),
          metadata: {
            source: result.source,
            latency: result.latency
          }
        }
      };
    });

    const batchResults = await Promise.all(batchPromises);
    const endTime = process.hrtime.bigint();
    const totalLatency = Number(endTime - startTime) / 1000000;

    // Add all messages to session
    for (const { user, assistant } of batchResults) {
      session.messages.push(user);
      session.messages.push(assistant);
    }

    // Send batch results
    ws.send(JSON.stringify({
      type: 'batch-complete',
      data: {
        results: batchResults,
        performance: {
          totalLatency: parseFloat(totalLatency.toFixed(2)),
          averageLatency: parseFloat((totalLatency / messages.length).toFixed(2)),
          sources: batchResults.map(r => r.assistant.metadata.source)
        }
      }
    }));
  }

  // Create stream from string
  private async* createStringStream(text: string): AsyncIterable<string> {
    const chunkSize = 1; // Ultra-fast single character chunks
    for (let i = 0; i < text.length; i += chunkSize) {
      yield text.slice(i, i + chunkSize);
      // No delay for maximum speed
    }
  }

  async start(port: number = 8080): Promise<void> {
    try {
      console.log('ðŸš€ Initializing ultra-fast services...');

      // Initialize all services in parallel for maximum speed
      await Promise.all([
        this.monitoringService.initializeMetrics(),
        this.cacheService.initializeCache(),
        this.preGenService.preGenerateResponses()
      ]);

      // Pre-warm connection pools
      await this.connectionPool.getStats();

      // Start pre-generation of common patterns
      setTimeout(() => {
        this.parallelProcessor.preGenerateResponses();
      }, 10000); // 10 seconds after start

      // Start server
      this.server.listen(port, () => {
        console.log(`ðŸš€ Ultra-Fast Hyper-Realistic AI Chatbot Server running on port ${port}`);
        console.log(`âš¡ Response speed: Up to 20,000 tokens/second`);
        console.log(`ðŸ“Š WebSocket endpoint: ws://localhost:${port}/ws/[sessionId]`);
        console.log(`ðŸ” Health check: http://localhost:${port}/health`);
        console.log(`ðŸ“ˆ Metrics: http://localhost:${port}/api/metrics`);
        console.log(`ðŸŽ¯ Optimization stats: http://localhost:${port}/api/optimization/stats`);
        console.log(`ðŸ”® Predictive API: http://localhost:${port}/api/predict`);
        console.log(`âš¡ Batch processing: http://localhost:${port}/api/sessions/[id]/batch`);
      });

      // Cleanup old data periodically
      setInterval(() => {
        this.monitoringService.cleanup();
        this.cacheService.cleanup();
        this.ultraFastMiddleware.cleanup();
      }, 60 * 60 * 1000); // Every minute

      // Save learned patterns periodically
      setInterval(() => {
        this.predictionService.saveLearnedPatterns();
      }, 5 * 60 * 1000); // Every 5 minutes

    } catch (error: any) {
      console.error('Failed to start server:', error);
      process.exit(1);
    }
  }

  async stop(): Promise<void> {
    return new Promise((resolve) => {
      this.server.close(() => {
        console.log('Server stopped');
        resolve();
      });
    });
  }
}